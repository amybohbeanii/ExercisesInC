## Caching

### What happens if a program writes a new value into the program counter?
  * PC contains the address in memory of the next instruction. If a new value is written, then CPU fetches this instruction and stores in instruction register, decodes instruction sends signals, signals cause computation to occur 
### What is the fundamental problem caches are meant to solve?
  * Performance issues to complete instructions. The fundamental problems of computer architecture, the memory bottleneck.
### If cache access time is 1 ns and memory access time is 10 ns, what is the average access time of a program with hit rate 50%? How about 90%?
  * = cache access time + (1- hit ratio)(main access time) 
  * = 1 + (1-.5)(10) = 6ns
  * = 1 + (1-.9)(10) = 2ns
  * = h(Th) + m(Tm) = .5(1)+.5(10) = 5.5ns
  * = .9(1)+.1(10) = 1.9ns
### The book gives several examples of programming language features, like loops, that tend to improve locality in the access pattern of instructions and/or data. Can you think of other examples? Or counter-examples that might decrease locality?
  * http://stackoverflow.com/questions/763262/how-does-one-write-code-that-best-utilizes-the-cpu-cache-to-improve-performance
  * use smaller data types, avoid data structures that have irregular access patterns, iterate through multidimensional arrays efficiently, recusion. When functions call other functions further away can decrease locality.
### If you refactor a program to improve locality, would you say the program is "cache aware"? Why not?
  * If a program has improved locality, then yes, it is cache aware because information is stored more effectively in the cache by having information closer.
### See if you can estimate the cost of a memory cache by comparing the prices of two similar CPUs with different cache sizes.
  * https://www.amazon.com/gp/product/B06W9JXK4G/ref=s9_acsd_newrz_hd_bw_bxcb_c_x_1_w?pf_rd_m=ATVPDKIKX0DER&pf_rd_s=merchandised-search-4&pf_rd_r=0DNNX75E9ZTW0E0BV66Z&pf_rd_t=101&pf_rd_p=1381e980-855e-537b-94df-b4f4d642d410&pf_rd_i=229189
  * Not sure if comparable but theres different levels of cache. This one as 4mb L1 and 8mb L2 but 500/20MB = ~$25/per MB
  * https://www.amazon.com/gp/product/B01MXSI216/ref=s9_acsd_top_hd_bw_bxcb_c_x_1_w?pf_rd_m=ATVPDKIKX0DER&pf_rd_s=merchandised-search-3&pf_rd_r=0DNNX75E9ZTW0E0BV66Z&pf_rd_t=101&pf_rd_p=1381e980-855e-537b-94df-b4f4d642d410&pf_rd_i=229189
  * 346/8MB of smart cache = ~$42/per MB
### Why are cache policies generally more complex at the bottom of the memory hierarchy?
  * Becuase the bottom of memory hierarchy take a long time to process/to make decisions, and a longer proccessing time means having well designed policies can make a big difference.  
### Can you think of a strategy operating systems could use to avoid thrashing or recover when it occurs?
  * operating systems could avoid thrashing by detecting an increase in paging and blocking or killing processes until the system is responsive again
### Run the cache code on your laptop or another computer and see if you can infer the cache size and block size. If you can find the technical specifications for your computer, see if your inferences are right.
  
### In this directory you should find a subdirectory named cache that contains cache.c and supporting files. Read cache.c, then run make and ./cache > data. Run python graph_data.py to see the results. What can you infer about the cache structure on your computer
 *![Image of cache](https://github.com/amybohbeanii/ExercisesInC/blob/master/thinkos/cache.png)
 * cache performance is good until 2^21 B which is about 2MiB. I can infer that the cache size is probably 2MiB
