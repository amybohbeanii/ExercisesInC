#Caching

What happens if a program writes a new value into the program counter?

What is the fundamental problem caches are meant to solve?

If cache access time is 1 ns and memory access time is 10 ns, what is the average access time of a program with hit rate 50%? How about 90%?

The book gives several examples of programming language features, like loops, that tend to improve locality in the access pattern of instructions and/or data. Can you think of other examples?
Or counter-examples that might decrease locality?

If you refactor a program to improve locality, would you say the program is "cache aware"? Why not?

See if you can estimate the cost of a memory cache by comparing the prices of two similar CPUs with different cache sizes.

Why are cache policies generally more complex at the bottom of the memory hierarchy?

Can you think of a strategy operating systems could use to avoid thrashing or recover when it occurs?

Run the cache code on your laptop or another computer and see if you can infer the cache size and block size.
If you can find the technical specifications for your computer, see if your inferences are right.

In this directory you should find a subdirectory named cache that contains cache.c and supporting files. Read cache.c, then run make and ./cache.

Run python graph_data.py to see the results. What can you infer about the cache structure on your computer
